{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1iJ7XJshmULjPmIk8B03C"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1.Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
      ],
      "metadata": {
        "id": "Qy_-K-RAmzHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Linear Regression:\n",
        "\n",
        "Uses one independent variable to predict the dependent variable.\n",
        "Models a linear relationship between the two variables, meaning the change in the dependent variable is constant for each unit change in the independent variable.\n",
        "\n",
        "Example: Predicting house prices based on square footage. You would model the relationship between house price (y) and square footage (x).\n",
        "\n",
        "Multiple Linear Regression:\n",
        "\n",
        "Uses two or more independent variables to predict the dependent variable.\n",
        "Models a linear relationship between the dependent variable and a combination of the independent variables.\n",
        "\n",
        "Example: Predicting student exam scores based on study hours, prior knowledge, and test difficulty. You would model the relationship between exam score (y) and study hours (x1), prior knowledge (x2), and test difficulty (x3)."
      ],
      "metadata": {
        "id": "AeiJZthfm0w8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
      ],
      "metadata": {
        "id": "buZWbIRypzbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Linearity: This assumption states that the relationship between the independent and dependent variables is linear. In other words, there's a straight-line relationship between the variables.\n",
        "\n",
        "Evaluation: Plot your data as a scatterplot. Look for patterns like curves, bends, or clusters instead of a straight line. You can also use statistical tests like linearity tests (e.g., F-test for linearity) to assess the linearity formally.\n",
        "\n",
        "Independence of Errors: This assumption states that the errors for each data point are independent of each other and not influenced by other observations. In other words, the error for one point shouldn't affect the error for another.\n",
        "\n",
        "Evaluation: Check for patterns in the residual plot over time (if time-series data) or against other variables. Statistical tests like Durbin-Watson test can help detect autocorrelation (dependencies between errors)."
      ],
      "metadata": {
        "id": "Yb0-zfLar8nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
      ],
      "metadata": {
        "id": "NSx9dTZEvjTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Interpreting the slope and intercept in a linear regression model is essential for understanding the relationship between the independent variable(s) and the dependent variable\n",
        "\n",
        "Intercept (β0):\n",
        "\n",
        "Represents the predicted value of the dependent variable when all independent variables are equal to zero.\n",
        "\n",
        "Slope (βi):\n",
        "\n",
        "Represents the average change in the dependent variable for a one-unit increase in the corresponding independent variable, holding all other independent variables constant.\n",
        "\n",
        "Positive slope: Indicates a direct relationship, meaning as the independent variable increases, the dependent variable also increases.\n",
        "\n",
        "Negative slope: Indicates an inverse relationship, meaning as the independent variable increases, the dependent variable decreases.\n",
        "\n",
        "Example:\n",
        "\n",
        "Scenario: Predicting house prices based on square footage.\n",
        "\n",
        "Model equation: Price = β0 + β1 * SquareFootage + ε\n",
        "\n",
        "Interpretation:\n",
        "β0: Doesn't have a direct meaning in this context. It might represent the average price of a house with zero square footage, which is impossible.\n",
        "\n",
        "β1: Represents the average change in price per square foot. For example, if β1 is 100, then houses with an extra square foot are predicted to be $100 more expensive on average."
      ],
      "metadata": {
        "id": "ErGSCrHMvkdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4.Explain the concept of gradient descent. How is it used in machine learning?"
      ],
      "metadata": {
        "id": "Dgl9ub9ixmt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Math Terms:\n",
        "\n",
        "We have a cost function, which measures how well our guess fits the data.\n",
        "\n",
        "The gradient tells us the direction of the steepest decrease in the cost function.\n",
        "\n",
        "We update our guess by subtracting a small multiple of the gradient, effectively moving downhill.\n",
        "\n",
        "Gradient Descent in Machine Learning\n",
        "\n",
        "Machine learning models learn by adjusting their internal parameters to minimize a cost function, often representing the error between predictions and actual outcomes. This is where gradient descent comes in:\n",
        "\n",
        "The cost function: We define a function that measures how well the model performs on the training data.\n",
        "\n",
        "Initial parameters: We start with random values for the model's parameters.\n",
        "\n",
        "Gradient calculation: We calculate the gradient of the cost function with respect to each parameter."
      ],
      "metadata": {
        "id": "sZbJQcUNxm18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
      ],
      "metadata": {
        "id": "NQgrpIEd4TD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Linear Regression has 2 or more variables.\n",
        "\n",
        "Multiple Linear Regression models are more complex and capture multi dimension relationships.\n",
        "\n",
        "Multiple Regression shows the indivisual and combined effect of multiple variables.\n",
        "\n",
        "The applications of Multiple Linear Regression are on a broader scale."
      ],
      "metadata": {
        "id": "HiuEspuF4TL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
      ],
      "metadata": {
        "id": "-VSGLNql8gwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Mulitiple Linear Regression when multiple independent variables predict a dependent variable multicollinearity arises.\n",
        "\n",
        "Multicollinearity can tend to overfit the model which can show performance decrease on new data.\n",
        "\n",
        "We can detect multicollinearity by plotting a correlation matrix and check for  values above 0.8.\n",
        "\n",
        "To address this issue we can remove a redundant variable.\n",
        "\n",
        "Another technique that can be used is Dimensionality Reduction. One of the popular dimension reducationality technqiues is Principal Component Analysis.\n",
        "where we reduce the number of features while preserving important information.\n"
      ],
      "metadata": {
        "id": "PFJ0nXrL8g0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Describe the polynomial regression model. How is it different from linear regression?"
      ],
      "metadata": {
        "id": "nWrNVBbxBvCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression can be used when the relationship between the independent and dependent variables is complex.\n",
        "\n",
        "Aims to capture relations between the independent and dependent variables which the simple LR model would not be able to do so.\n",
        "\n",
        "Polynomial regression adapts to non linear patterns.\n",
        "\n",
        "One of the downsides of using polynomial regression is that it can be prone to overfitting.\n"
      ],
      "metadata": {
        "id": "tIoVFxjXBt6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
      ],
      "metadata": {
        "id": "KuKtKTQNpwhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages:\n",
        "\n",
        "Polynomial regression adapts to non linear patterns.\n",
        "\n",
        "Offers flexibility to model different curve shapes by adjusting the polynomial degree.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Prone to overfitting\n",
        "\n",
        "Can be sensitive to outliers and noise in the data.\n",
        "\n",
        "It would be a good option to choose polynomial regression in a situation where the relationship between the independent and dependent variables is complex and the data is non linear."
      ],
      "metadata": {
        "id": "5QDUmFV3pwKN"
      }
    }
  ]
}