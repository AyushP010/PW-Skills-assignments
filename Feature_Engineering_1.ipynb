{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzftetawjT+d9TBPKlqBO1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1.What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
      ],
      "metadata": {
        "id": "UxyB2F6fUNZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing values are values which are not present in a dataset. it is essential to handle missing values because if not handled at the right time the accuracy of the model can be impacted drastically.\n",
        "\n",
        "a few reasons to handle missing data:\n",
        "\n",
        "Bias and Inaccuracy: Missing data can introduce bias and inaccuracies in the analysis and modeling process, leading to incorrect conclusions.\n",
        "\n",
        "Reduced Power and Precision: Missing values can reduce the statistical power and precision of analyses, making it challenging to draw reliable conclusions.\n",
        "\n",
        "Several algorithms are not affected by missing values, or they can handle them without requiring explicit imputation. Some of these algorithms include:\n",
        "\n",
        "Decision Trees: Decision trees can naturally handle missing values during the training and prediction phases. They determine the best split based on available data.\n",
        "\n",
        "Random Forests: Random Forests, being an ensemble of decision trees, inherit the ability to handle missing values from decision trees."
      ],
      "metadata": {
        "id": "6UBrju6xUNfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.List down techniques used to handle missing data. Give an example of each with python code."
      ],
      "metadata": {
        "id": "K0wg9i7VbVxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling missing data is a crucial step in data preprocessing. Here are some common techniques along with examples in Python:\n",
        "\n",
        "Dropping Missing Values:\n",
        "This involves removing rows or columns with missing values.\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "Sample DataFrame with missing values\n",
        "\n",
        "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "Drop rows with missing values\n",
        "\n",
        "df_dropped_rows = df.dropna()\n",
        "\n",
        "print(\"DataFrame after dropping rows with missing values:\")\n",
        "\n",
        "print(df_dropped_rows)\n",
        "\n",
        "Drop columns with missing values\n",
        "\n",
        "df_dropped_columns = df.dropna(axis=1)\n",
        "\n",
        "print(\"\\nDataFrame after dropping columns with missing values:\")\n",
        "\n",
        "print(df_dropped_columns)\n"
      ],
      "metadata": {
        "id": "fQOpvv_PbV1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean/Median Imputation:\n",
        "\n",
        "Fill missing values with the mean or median of the column.\n",
        "\n",
        "Mean imputation\n",
        "\n",
        "df_mean_imputed = df.fillna(df.mean())\n",
        "\n",
        "print(\"DataFrame after mean imputation:\")\n",
        "\n",
        "print(df_mean_imputed)\n",
        "\n",
        "Median imputation\n",
        "\n",
        "df_median_imputed = df.fillna(df.median())\n",
        "\n",
        "print(\"\\nDataFrame after median imputation:\")\n",
        "\n",
        "print(df_median_imputed)\n"
      ],
      "metadata": {
        "id": "QU2pnzR3PsiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Explain imbalanced data. What will happen if imbalanced data is not handled in a dataset?"
      ],
      "metadata": {
        "id": "hLTCY34nTsEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imbalanced data normally arises in a classification problem. It happens when the distribution of classes is not proper.\n",
        "\n",
        "problems that can arise if imbalanced data is not handled:\n",
        "\n",
        "Unfair Models:\n",
        "\n",
        "Given that predicting the majority class more often might nevertheless result in high accuracy, the model may be biased in favor of the majority class. If the minority class is the one that is of interest (e.g., detecting fraud, identifying uncommon illnesses), then this becomes problematic.\n",
        "\n",
        "Poor Generalization\n",
        "\n",
        "Poor performance on fresh, unseen data from the minority class might result from the model's poor generalization to that class.\n",
        "\n"
      ],
      "metadata": {
        "id": "fONt9tChTsQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
      ],
      "metadata": {
        "id": "2c7zAmXQXG_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Techniques like up- and down-sampling are employed to rectify imbalances in datasets by modifying the distribution of classes. These techniques are especially pertinent in situations when one class greatly outnumbers the others.\n",
        "\n",
        "Up-sampling (Over-sampling):\n",
        "\n",
        "Up-sampling involves increasing the number of instances in the minority class by generating synthetic samples or replicating existing ones. The goal is to balance the class distribution.\n"
      ],
      "metadata": {
        "id": "cDSxB3-FXHDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic imbalanced dataset\n",
        "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9],\n",
        "                           n_informative=3, n_redundant=1, flip_y=0,\n",
        "                           n_features=20, n_clusters_per_class=1,\n",
        "                           n_samples=1000, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply SMOTE for up-sampling\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "6uWQpE0reiLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Down Sampling: this mainly involves reducing the number of instances in a class.\n",
        "the goal is to balance the class distribution."
      ],
      "metadata": {
        "id": "ulWddTOpesm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic imbalanced dataset\n",
        "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.9, 0.1],\n",
        "                           n_informative=3, n_redundant=1, flip_y=0,\n",
        "                           n_features=20, n_clusters_per_class=1,\n",
        "                           n_samples=1000, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Random Under-sampling\n",
        "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "ImiI0Q51faZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5: What is data Augmentation? Explain SMOTE."
      ],
      "metadata": {
        "id": "1dhdPRzV_q8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through the application of different alterations to the current data, a technique known as \"data augmentation\" allows one to artificially expand the size of a dataset. This is frequently used to improve model resilience and generalization in machine learning, particularly in computer vision problems."
      ],
      "metadata": {
        "id": "V5keFklJAD5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE (Synthetic Minority Over-sampling Technique) is a specific data augmentation technique designed to address imbalanced datasets, particularly in the context of classification problems with minority and majority classes. SMOTE focuses on the minority class and aims to generate synthetic instances to balance the class distribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "_vR7U-i7BKAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how SMOTE functions:\n",
        "\n",
        "Find Instances of the Minority Class: SMOTE finds the k-nearest neighbors of each instance in the feature space of the minority class.\n",
        "\n",
        "Synthetic Instance Generation: SMOTE creates synthetic instances for every minority class instance by interpolating between the instance and its k-nearest neighbors.\n",
        "\n",
        "Make a Balanced Dataset: By include the synthetic examples in the original dataset, the distribution of classes is made more equitable.\n"
      ],
      "metadata": {
        "id": "etCHM0U4B-zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What are outliers in a dataset? Why is it essential to handle outliers?"
      ],
      "metadata": {
        "id": "cSp08vpQWScF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers are data points which are very different from the overall dataset. Outliers have observations which fall outside the typical range of values and can disrupt the overall distribution of the data.\n",
        "\n",
        "It is very essential to handle outliers for reasons such as:\n",
        "\n",
        "1. Model perforamance: When outliers are present in a dataset the overall accuracy of the model goes down and models can be sensitive towards extreme values.\n",
        "\n",
        "2. Robustness: Many statistical and machine learning models assume certain properties of the data, such as normality. Outliers can violate these assumptions and make models less robust."
      ],
      "metadata": {
        "id": "GrG5Z7QouLHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
      ],
      "metadata": {
        "id": "IY9yBtlHwf_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing data can be a problem which needs to be resolved as soon as possible.\n",
        "\n",
        "Some techniques which can be used to handle missing data are:\n",
        "\n",
        "Interpolation: We can estimate the missing values based on the given values of the other data points.\n",
        "one commonly used method of interpolation is linear interpolation.\n",
        "\n",
        "```\n",
        "df['column_name'].interpolate(method='linear',inplace=True)\n",
        "```\n",
        "\n",
        "Rows can be deleted which contain mising values\n",
        "\n",
        "```\n",
        "df.dropna(axis=0,inplace=True)\n",
        "```\n",
        "\n",
        "Columns can also be deleted which have missing values\n",
        "\n",
        "```\n",
        "df.dropna(column_name=' ', axis=1, inplace=True)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S7aH0v2mwgDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8.You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
      ],
      "metadata": {
        "id": "Ko9a2Gg61f7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "whenever we are dealing with missing data it is important to know whether the missing data is very random,missing at random or missing not at random.\n",
        "\n",
        "the Seaborn Library can be useful to check for missing data using HeatMap\n",
        "```\n",
        "import seaborn as sns\n",
        "sns.heatmap(df.isnull(),cbar=False)\n",
        "```\n",
        "Create a correlation matrix to check for missing data\n",
        "```\n",
        "correlation_matrix=df.corr()\n",
        "correlation_missing=correlation_matrix['column_with_missing_value']\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JR5edLNG1gad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9.Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
        "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
      ],
      "metadata": {
        "id": "FnPjZZGvUPBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dealing with imbalanced datasets in medical diagnosis or any classification problem requires thoughtful strategies to ensure that the machine learning model can effectively learn and generalize across both classes.\n",
        "\n",
        "Use Appropriate Evaluation Metrics:\n",
        "\n",
        "Avoid relying solely on accuracy, as it can be misleading in imbalanced datasets. Instead, consider using metrics that provide a more comprehensive view of the model's performance:\n",
        "\n",
        "Precision and Recall: Especially relevant in medical diagnosis. Precision measures the accuracy of positive predictions, and recall (sensitivity)\n",
        "measures the ability of the model to capture all positive instances.\n",
        "\n",
        "F1 Score: A balance between precision and recall, suitable for imbalanced datasets.\n",
        "\n",
        "Confusion Matrix Analysis:\n",
        "\n",
        "Examine the confusion matrix to understand the distribution of true positives, false positives, true negatives, and false negatives. This can provide insights into where the model is making errors.\n",
        "\n",
        "```\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "TnKUwu83UPGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
      ],
      "metadata": {
        "id": "J2fyNeOSMVWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downsampling the majority class can be a helpful tactic when working with an imbalanced dataset in the context of assessing customer happiness, where the majority class reflects satisfied consumers.\n",
        "\n",
        "Methods which can be used are:\n",
        "\n",
        "Random Sampling: Randomly remove instances from the dataset to achieve a more balanced dataset.\n",
        "\n",
        "Clustering Centroids: Use clustering to find centroids and use them as representatives. this preserves the overall charactterstics.\n"
      ],
      "metadata": {
        "id": "mM_PIrjeMVoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
        "balance the dataset and up-sample the minority class?"
      ],
      "metadata": {
        "id": "hyahEknBRz0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we know an unbalanced dataset can introduce errors which can lead to lower accuracy of the ML model. Any rare events that occur have to be checked properly to avoid issues.\n",
        "\n",
        "Some steps that can be followed are:\n",
        "\n",
        "1. First the problem needs to be understood properly because any uncommon outcome which doesn't occur frequently needs to be examined thoroughly.\n",
        "\n",
        "2. The classes or categories which are present in the dataset need to be identified.\n",
        "\n",
        "3. Imbalance in the data needs to be noticed.\n",
        "\n",
        "To up-sample the minority class we can create a random duplication of the data where many instances of the minority classes are created."
      ],
      "metadata": {
        "id": "ZLU2-XTCRz_W"
      }
    }
  ]
}